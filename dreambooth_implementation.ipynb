{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd642018",
   "metadata": {},
   "source": [
    "# DreamBooth 实现\n",
    "\n",
    "这个 Notebook 实现了 DreamBooth 算法，用于微调 Stable Diffusion 模型以学习新的概念（例如特定对象或风格）。\n",
    "\n",
    "它包含以下部分：\n",
    "1.  **依赖项检查和导入**\n",
    "2.  **辅助函数定义** (数据集加载、稀有令牌查找、类别图像生成等)\n",
    "3.  **训练函数** (`dreambooth_training`)\n",
    "4.  **推理函数** (`inference`)\n",
    "5.  **配置和执行** (设置参数并运行训练或推理)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a5b36",
   "metadata": {},
   "source": [
    "## 1. 依赖项检查和导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa37d0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有基本依赖项已满足。\n",
      "Diffusers 和 Transformers 导入成功。\n",
      "PyTorch 版本: 2.7.0+cu128\n",
      "CUDA 可用: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# 首先检查依赖项\n",
    "def check_dependencies():\n",
    "    missing_deps = []\n",
    "    try:\n",
    "        import torch\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"torch\")\n",
    "    \n",
    "    try:\n",
    "        import accelerate\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"accelerate\")\n",
    "        \n",
    "    try:\n",
    "        import transformers\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"transformers\")\n",
    "        \n",
    "    try:\n",
    "        import diffusers\n",
    "    except ImportError:\n",
    "        missing_deps.append(\"diffusers\")\n",
    "    \n",
    "    return missing_deps\n",
    "\n",
    "missing = check_dependencies()\n",
    "if missing:\n",
    "    print(\"缺少必要的依赖项，请先安装：\")\n",
    "    print(f\"pip install {' '.join(missing)}\")\n",
    "    print(\"\\n如果遇到Flash Attention相关错误，请尝试：\")\n",
    "    print(\"pip install diffusers==0.19.3 transformers==4.30.2 accelerate xformers\")\n",
    "    # 在Notebook中，我们通常不直接退出，而是抛出错误或显示消息\n",
    "    raise ImportError(f\"缺少依赖: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"所有基本依赖项已满足。\")\n",
    "\n",
    "# 正常导入，现在有更好的错误处理\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    # 尝试导入主要模块\n",
    "    from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel\n",
    "    from transformers import CLIPTextModel, CLIPTokenizer\n",
    "    print(\"Diffusers 和 Transformers 导入成功。\")\n",
    "except ImportError as e:\n",
    "    print(f\"导入错误: {e}\")\n",
    "    print(\"\\n请尝试安装兼容版本的依赖项:\")\n",
    "    print(\"pip install diffusers==0.19.3 transformers==4.30.2 accelerate\")\n",
    "    print(\"如果您使用NVIDIA GPU，可以添加: xformers\")\n",
    "    raise e\n",
    "\n",
    "from accelerate import Accelerator\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2752b5c2",
   "metadata": {},
   "source": [
    "## 2. 辅助函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d348aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(self, instance_images_path, class_images_path, tokenizer, size=512, center_crop=True):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # 简化图像加载\n",
    "        self.instance_images = self._load_images(instance_images_path) if os.path.exists(instance_images_path) else []\n",
    "        self.class_images = self._load_images(class_images_path) if class_images_path and os.path.exists(class_images_path) else []\n",
    "        print(f\"加载了 {len(self.instance_images)} 张实例图像和 {len(self.class_images)} 张类别图像\")\n",
    "    \n",
    "    def _load_images(self, path):\n",
    "        images = []\n",
    "        if not path:\n",
    "            return images\n",
    "            \n",
    "        print(f\"从 {path} 加载图像...\")\n",
    "        for file in tqdm(os.listdir(path), desc=f\"加载 {os.path.basename(path)}\"):\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                try:\n",
    "                    img = Image.open(os.path.join(path, file)).convert('RGB')\n",
    "                    # 简化图像处理\n",
    "                    if self.center_crop:\n",
    "                        w, h = img.size\n",
    "                        min_dim = min(w, h)\n",
    "                        img = img.crop(((w-min_dim)//2, (h-min_dim)//2, (w+min_dim)//2, (h+min_dim)//2))\n",
    "                    img = img.resize((self.size, self.size))\n",
    "                    images.append(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"警告: 跳过无法加载的图像 {file}: {e}\")\n",
    "        return images\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 确保至少有一个实例图像\n",
    "        if not self.instance_images:\n",
    "             raise ValueError(\"错误: 实例图像目录为空或无法加载任何图像。请检查 instance_data_dir。\")\n",
    "        return max(len(self.instance_images), len(self.class_images))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        # 获取实例图像 (循环使用如果数量少于类别图像)\n",
    "        instance_image = self.instance_images[idx % len(self.instance_images)]\n",
    "        instance_image_tensor = torch.from_numpy(np.array(instance_image).astype(np.float32) / 127.5 - 1.0)\n",
    "        item[\"instance_pixel_values\"] = instance_image_tensor.permute(2, 0, 1)\n",
    "        \n",
    "        # 获取类别图像 (如果存在)\n",
    "        if self.class_images:\n",
    "            class_image = self.class_images[idx % len(self.class_images)]\n",
    "            class_image_tensor = torch.from_numpy(np.array(class_image).astype(np.float32) / 127.5 - 1.0)\n",
    "            item[\"class_pixel_values\"] = class_image_tensor.permute(2, 0, 1)\n",
    "            \n",
    "        return item\n",
    "\n",
    "def find_rare_token(tokenizer, token_range=(5000, 10000)):\n",
    "    \"\"\"\n",
    "    按照论文所述，查找稀有令牌作为标识符\n",
    "    对于Stable Diffusion，我们使用CLIP tokenizer\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        token_id = random.randint(token_range[0], token_range[1])\n",
    "        # 确保选择的是符合条件的token（3个或更少Unicode字符，不含空格）\n",
    "        token_text = tokenizer.decode([token_id]).strip()\n",
    "        if 1 <= len(token_text) <= 3 and ' ' not in token_text and token_text.isprintable():\n",
    "             # 进一步检查是否是特殊标记或常用词的一部分 (简单检查)\n",
    "            if not token_text.startswith('<') and not token_text.endswith('>') and len(token_text) > 1:\n",
    "                 return token_text\n",
    "\n",
    "def generate_class_images(model, class_prompt, output_dir, num_samples=200):\n",
    "    \"\"\"\n",
    "    生成类别图像以用于先验保留损失\n",
    "    这是论文中提到的先验保留机制的关键部分\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    else:\n",
    "        # 如果目录已存在且包含足够图片，则跳过生成\n",
    "        existing_files = [f for f in os.listdir(output_dir) if f.lower().endswith('.png')]\n",
    "        if len(existing_files) >= num_samples:\n",
    "            print(f\"已在 {output_dir} 找到 {len(existing_files)} 张类别图像，跳过生成。\")\n",
    "            return output_dir\n",
    "        else:\n",
    "             print(f\"在 {output_dir} 找到 {len(existing_files)} 张类别图像，需要生成 {num_samples - len(existing_files)} 张。\")\n",
    "             num_samples -= len(existing_files)\n",
    "             start_idx = len(existing_files)\n",
    "\n",
    "    print(f\"正在生成 {num_samples} 张类别图像用于先验保留到 {output_dir}...\")\n",
    "    \n",
    "    # 将模型设置为推理模式\n",
    "    model.safety_checker = None  # 禁用安全检查器以加快生成\n",
    "    device = model.device\n",
    "    \n",
    "    # 批量生成图像以加快速度\n",
    "    batch_size = 8 # 增加批量大小以提高效率\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    generated_count = 0\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"生成类别图像\"):\n",
    "        current_batch_size = min(batch_size, num_samples - generated_count)\n",
    "        if current_batch_size <= 0:\n",
    "            break\n",
    "        batch_prompts = [class_prompt] * current_batch_size\n",
    "        with torch.no_grad():\n",
    "            # 确保模型在正确的设备上\n",
    "            if model.device != device:\n",
    "                 model.to(device)\n",
    "            outputs = model(batch_prompts, num_inference_steps=50, guidance_scale=7.5)\n",
    "            \n",
    "        for i, image in enumerate(outputs.images):\n",
    "            img_idx = start_idx + generated_count + i\n",
    "            image.save(os.path.join(output_dir, f\"class_{img_idx:04d}.png\"))\n",
    "        generated_count += len(outputs.images)\n",
    "            \n",
    "    print(f\"成功生成 {generated_count} 张类别图像。\")\n",
    "    return output_dir\n",
    "\n",
    "def download_small_model():\n",
    "    \"\"\"\n",
    "    自动下载小型模型，适合低资源设备\n",
    "    返回模型的路径或名称\n",
    "    \"\"\"\n",
    "    print(\"选择适合低资源设备的小型模型...\")\n",
    "    \n",
    "    # 定义小型模型选项 - 更新为较旧但稳定且兼容性更好的版本\n",
    "    small_models = [\n",
    "        \"CompVis/stable-diffusion-v1-4\",                # 较旧但稳定的SD1.4，兼容性好\n",
    "        \"runwayml/stable-diffusion-v1-5\",               # 标准SD1.5\n",
    "        \"stabilityai/stable-diffusion-2-base\",          # SD2基础版\n",
    "    ]\n",
    "    \n",
    "    # 选择默认小型模型\n",
    "    chosen_model = small_models[0]\n",
    "    \n",
    "    print(f\"已选择模型: {chosen_model}\")\n",
    "    print(f\"此模型与较旧版本的diffusers兼容性更好\")\n",
    "    print(\"模型将在首次使用时自动从Hugging Face下载\")\n",
    "    \n",
    "    return chosen_model\n",
    "\n",
    "# 添加这个函数来显示简洁的使用说明\n",
    "def show_quick_help():\n",
    "    \"\"\"显示简洁的使用说明\"\"\"\n",
    "    print(\"\\n请在下方的配置单元格中设置参数并运行后续单元格。\")\n",
    "    print(\"主要参数:\")\n",
    "    print(\"  train_mode = True  # 或 False\")\n",
    "    print(\"  infer_mode = True  # 或 False\")\n",
    "    print(\"  instance_data_dir = './my_images' # 包含你的训练图片的目录\")\n",
    "    print(\"  class_prompt = 'a photo of a dog' # 你的训练对象所属的类别\")\n",
    "    print(\"  output_dir = './output' # 模型输出目录\")\n",
    "    print(\"  prompt = 'a photo of a sks dog on the beach' # 推理时使用的提示词\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d8104",
   "metadata": {},
   "source": [
    "## 3. 训练函数 (`dreambooth_training`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d26366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dreambooth_training(\n",
    "    pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\",\n",
    "    instance_data_dir=\"./instance_images\",\n",
    "    output_dir=\"./output\",\n",
    "    class_prompt=\"a dog\",\n",
    "    instance_prompt_template=\"a photo of a {} dog\", # 使用模板，{} 会被稀有令牌替换\n",
    "    learning_rate=5e-6,\n",
    "    max_train_steps=800, # 减少默认步数以加快示例运行\n",
    "    prior_preservation_weight=1.0,  # λ参数，控制先验保留损失的权重\n",
    "    prior_generation_samples=50, # 减少默认类别图像数量以节省时间和资源\n",
    "    gradient_accumulation_steps=1,\n",
    "    train_text_encoder=True,  # 是否微调文本编码器\n",
    "    train_batch_size=1,\n",
    "    seed=42,\n",
    "    mixed_precision=\"fp16\" # 默认启用混合精度\n",
    "):\n",
    "    # 设置种子以保证结果可复现\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 初始化加速器\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=mixed_precision\n",
    "    )\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    try:\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path, \n",
    "            subfolder=\"tokenizer\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "         print(f\"加载 Tokenizer 失败: {e}\")\n",
    "         print(f\"尝试不指定 subfolder 加载...\")\n",
    "         tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "    \n",
    "    # 查找稀有令牌作为标识符，如论文3.2节所述\n",
    "    identifier = find_rare_token(tokenizer)\n",
    "    print(f\"选中的稀有令牌标识符: '{identifier}'\")\n",
    "    \n",
    "    # 构建实例提示词\n",
    "    instance_prompt = instance_prompt_template.format(identifier)\n",
    "    \n",
    "    print(f\"实例提示词: '{instance_prompt}'\")\n",
    "    print(f\"类别提示词: '{class_prompt}'\")\n",
    "    \n",
    "    # 加载预训练模型\n",
    "    print(f\"加载预训练模型: {pretrained_model_name_or_path}\")\n",
    "    try:\n",
    "        # 加载文本编码器\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            pretrained_model_name_or_path, \n",
    "            subfolder=\"text_encoder\"\n",
    "        )\n",
    "        # 加载U-Net\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            subfolder=\"unet\",\n",
    "        )\n",
    "        # 加载用于生成类别图像的完整pipeline\n",
    "        pipeline_for_gen = StableDiffusionPipeline.from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "        )\n",
    "    except Exception as e:\n",
    "         print(f\"加载模型组件失败: {e}\")\n",
    "         print(f\"尝试不指定 subfolder 加载...\")\n",
    "         text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path)\n",
    "         unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path)\n",
    "         pipeline_for_gen = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path)\n",
    "\n",
    "    pipeline_for_gen.to(accelerator.device)\n",
    "    \n",
    "    # 生成类别图像用于先验保留\n",
    "    class_images_dir = os.path.join(output_dir, \"class_images\")\n",
    "    generate_class_images(pipeline_for_gen, class_prompt, class_images_dir, prior_generation_samples)\n",
    "    # 释放生成模型的显存\n",
    "    del pipeline_for_gen\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # 加载数据集\n",
    "    dataset = DreamBoothDataset(\n",
    "        instance_images_path=instance_data_dir,\n",
    "        class_images_path=class_images_dir,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=train_batch_size, shuffle=True, num_workers=0) # num_workers=0 避免多进程问题\n",
    "    \n",
    "    # 准备优化器\n",
    "    params_to_optimize = list(unet.parameters())\n",
    "    if train_text_encoder:\n",
    "        print(\"将同时微调U-Net和文本编码器\")\n",
    "        params_to_optimize += list(text_encoder.parameters())\n",
    "        # 确保文本编码器需要梯度\n",
    "        text_encoder.requires_grad_(True)\n",
    "    else:\n",
    "        print(\"仅微调U-Net，冻结文本编码器\")\n",
    "        text_encoder.requires_grad_(False)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params_to_optimize,\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "    )\n",
    "    \n",
    "    # 准备噪声调度器\n",
    "    try:\n",
    "        noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "            pretrained_model_name_or_path, \n",
    "            subfolder=\"scheduler\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "         print(f\"加载 Scheduler 失败: {e}\")\n",
    "         print(f\"尝试不指定 subfolder 加载...\")\n",
    "         noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path)\n",
    "    \n",
    "    # 准备文本嵌入 (移到训练循环内部，因为文本编码器可能被训练)\n",
    "    # instance_text_inputs = tokenizer(...)\n",
    "    # class_text_inputs = tokenizer(...)\n",
    "    \n",
    "    # 将模型、优化器和数据加载器准备用于加速训练\n",
    "    if train_text_encoder:\n",
    "        unet, text_encoder, optimizer, dataloader = accelerator.prepare(\n",
    "            unet, text_encoder, optimizer, dataloader\n",
    "        )\n",
    "    else:\n",
    "         # 如果不训练文本编码器，则不需要 prepare 它\n",
    "        unet, optimizer, dataloader = accelerator.prepare(\n",
    "            unet, optimizer, dataloader\n",
    "        )\n",
    "        # 将文本编码器移到GPU\n",
    "        text_encoder.to(accelerator.device)\n",
    "    \n",
    "    # 训练循环\n",
    "    print(\"开始训练...\")\n",
    "    progress_bar = tqdm(range(max_train_steps), desc=\"训练进度\", disable=not accelerator.is_local_main_process)\n",
    "    global_step = 0\n",
    "    \n",
    "    # 计算文本嵌入的函数\n",
    "    def get_text_embeddings(prompt_text):\n",
    "        text_inputs = tokenizer(\n",
    "            prompt_text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.to(accelerator.device)\n",
    "        return text_encoder(text_inputs)[0]\n",
    "\n",
    "    # 使用 accelerator.main_process_first 确保主进程先下载/缓存模型\n",
    "    with accelerator.main_process_first():\n",
    "        # 预计算类别提示词嵌入 (如果文本编码器不训练)\n",
    "        if not train_text_encoder:\n",
    "            with torch.no_grad():\n",
    "                precomputed_class_embeds = get_text_embeddings(class_prompt)\n",
    "        else:\n",
    "            precomputed_class_embeds = None\n",
    "\n",
    "    while global_step < max_train_steps:\n",
    "        unet.train()\n",
    "        if train_text_encoder:\n",
    "            text_encoder.train()\n",
    "        \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "                \n",
    "            with accelerator.accumulate(unet):\n",
    "                # 准备输入\n",
    "                instance_pixel_values = batch[\"instance_pixel_values\"].to(accelerator.device)\n",
    "                batch_size = instance_pixel_values.shape[0]\n",
    "                \n",
    "                # 获取文本嵌入\n",
    "                with torch.set_grad_enabled(train_text_encoder):\n",
    "                    encoder_hidden_states_instance = get_text_embeddings(instance_prompt)\n",
    "                    if train_text_encoder:\n",
    "                        encoder_hidden_states_class = get_text_embeddings(class_prompt)\n",
    "                    else:\n",
    "                        # 使用预计算的嵌入\n",
    "                        encoder_hidden_states_class = precomputed_class_embeds\n",
    "                \n",
    "                # 为实例图像添加噪声\n",
    "                noise_instance = torch.randn_like(instance_pixel_values)\n",
    "                timesteps_instance = torch.randint(\n",
    "                    0, \n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (batch_size,), \n",
    "                    device=accelerator.device\n",
    "                ).long()\n",
    "                noisy_images_instance = noise_scheduler.add_noise(instance_pixel_values, noise_instance, timesteps_instance)\n",
    "                \n",
    "                # 预测实例噪声残差\n",
    "                noise_pred_instance = unet(noisy_images_instance, timesteps_instance, encoder_hidden_states_instance).sample\n",
    "                instance_loss = F.mse_loss(noise_pred_instance.float(), noise_instance.float(), reduction=\"mean\")\n",
    "                \n",
    "                # 处理类别样本（先验保留）\n",
    "                class_loss = torch.tensor(0.0, device=accelerator.device, dtype=instance_loss.dtype)\n",
    "                if \"class_pixel_values\" in batch and prior_preservation_weight > 0:\n",
    "                    class_pixel_values = batch[\"class_pixel_values\"].to(accelerator.device)\n",
    "                    noise_class = torch.randn_like(class_pixel_values)\n",
    "                    timesteps_class = torch.randint(\n",
    "                        0, \n",
    "                        noise_scheduler.config.num_train_timesteps,\n",
    "                        (batch_size,), \n",
    "                        device=accelerator.device\n",
    "                    ).long()\n",
    "                    noisy_images_class = noise_scheduler.add_noise(class_pixel_values, noise_class, timesteps_class)\n",
    "                    \n",
    "                    # 预测类别噪声残差\n",
    "                    noise_pred_class = unet(noisy_images_class, timesteps_class, encoder_hidden_states_class).sample\n",
    "                    class_loss = F.mse_loss(noise_pred_class.float(), noise_class.float(), reduction=\"mean\")\n",
    "                \n",
    "                # 组合损失\n",
    "                loss = instance_loss + prior_preservation_weight * class_loss\n",
    "                \n",
    "                # 反向传播\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                # 梯度裁剪 (可选但推荐)\n",
    "                if accelerator.sync_gradients:\n",
    "                    params_to_clip = list(unet.parameters())\n",
    "                    if train_text_encoder:\n",
    "                        params_to_clip += list(text_encoder.parameters())\n",
    "                    accelerator.clip_grad_norm_(params_to_clip, 1.0)\n",
    "                \n",
    "                # 优化步骤\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            # 检查是否需要同步梯度\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                \n",
    "            # 记录和打印进度\n",
    "            logs = {\"loss\": loss.detach().item(), \"instance_loss\": instance_loss.detach().item(), \"class_loss\": class_loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            # accelerator.log(logs, step=global_step) # 如果配置了 tracker\n",
    "            \n",
    "    # 等待所有进程完成\n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    # 保存微调后的模型 (仅在主进程执行)\n",
    "    if accelerator.is_main_process:\n",
    "        unet = accelerator.unwrap_model(unet)\n",
    "        if train_text_encoder:\n",
    "            text_encoder = accelerator.unwrap_model(text_encoder)\n",
    "        \n",
    "        # 创建输出目录\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # 从原始pipeline创建新的pipeline\n",
    "        # 需要重新加载原始模型以获取 vae 和 scheduler 等其他组件\n",
    "        try:\n",
    "            pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "                pretrained_model_name_or_path,\n",
    "                unet=unet,\n",
    "                text_encoder=text_encoder,\n",
    "                tokenizer=tokenizer, # 包含 tokenizer\n",
    "            )\n",
    "        except Exception as e:\n",
    "             print(f\"从原始模型创建 Pipeline 失败: {e}\")\n",
    "             print(f\"尝试不指定 subfolder 加载...\")\n",
    "             pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "                 pretrained_model_name_or_path,\n",
    "                 unet=unet,\n",
    "                 text_encoder=text_encoder,\n",
    "                 tokenizer=tokenizer,\n",
    "             )\n",
    "        \n",
    "        # 保存微调后的模型\n",
    "        pipeline.save_pretrained(output_dir)\n",
    "        print(f\"模型已保存到 {output_dir}\")\n",
    "        \n",
    "        # 保存标识符以供将来推理使用\n",
    "        with open(os.path.join(output_dir, \"identifier.txt\"), \"w\") as f:\n",
    "            f.write(identifier)\n",
    "        print(f\"标识符 '{identifier}' 已保存到 {os.path.join(output_dir, 'identifier.txt')}\")\n",
    "    \n",
    "    # 清理显存\n",
    "    del unet, text_encoder, optimizer, dataloader, dataset, accelerator\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # 返回标识符，因为 pipeline 对象可能很大且已释放\n",
    "    return identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb9e32",
   "metadata": {},
   "source": [
    "## 4. 推理函数 (`inference`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264326aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model_path=\"./output\",\n",
    "    prompt_template=\"a photo of a {} dog wearing a hat\", # 推理提示词模板\n",
    "    class_prompt=\"a dog\", # 用于提取类别名词\n",
    "    identifier=None,\n",
    "    output_image_path=\"./generated_image.png\",\n",
    "    num_images=1,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=50,\n",
    "    seed=None # 推理时允许随机种子\n",
    "):\n",
    "    # 确保目录存在\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"错误: 模型路径不存在: {model_path}\")\n",
    "        print(\"请确保您已经训练了模型或提供了正确的路径\")\n",
    "        return None\n",
    "\n",
    "    # 改进的设备选择\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"正在加载模型到设备: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # 加载微调后的模型\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(model_path)\n",
    "        pipeline = pipeline.to(device)\n",
    "        \n",
    "        # 如果有CUDA，尝试启用内存优化\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                # 尝试启用内存优化\n",
    "                pipeline.enable_attention_slicing()\n",
    "                print(\"已启用 Attention Slicing 优化\")\n",
    "            except Exception as e:\n",
    "                print(f\"注意: 无法启用 Attention Slicing: {e}\")\n",
    "            try:\n",
    "                # 尝试检测并启用xFormers优化\n",
    "                pipeline.enable_xformers_memory_efficient_attention()\n",
    "                print(\"已启用 xFormers 优化以提高性能\")\n",
    "            except Exception as e:\n",
    "                print(f\"注意: 无法启用 xFormers: {e} (请确保已安装 xformers)\")\n",
    "    \n",
    "        # 如果未提供标识符但存在标识符文件，则读取它\n",
    "        if identifier is None and os.path.exists(os.path.join(model_path, \"identifier.txt\")):\n",
    "            with open(os.path.join(model_path, \"identifier.txt\"), \"r\") as f:\n",
    "                identifier = f.read().strip()\n",
    "                print(f\"从文件加载标识符: {identifier}\")\n",
    "        \n",
    "        # 如果未提供提示词，则使用标识符创建\n",
    "        if identifier is None:\n",
    "             raise ValueError(\"错误: 无法找到或确定标识符 (identifier)。请确保模型已训练或手动提供 --identifier 参数。\")\n",
    "        \n",
    "        # 使用模板构建最终提示词\n",
    "        prompt = prompt_template.format(identifier)\n",
    "        \n",
    "        # 设置生成器以控制随机性\n",
    "        generator = None\n",
    "        if seed is not None:\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "            print(f\"使用种子 {seed} 进行推理\")\n",
    "        else:\n",
    "             print(\"使用随机种子进行推理\")\n",
    "\n",
    "        # 生成图像\n",
    "        print(f\"使用提示词生成图像: '{prompt}'\")\n",
    "        \n",
    "        # 将生成过程包装在 no_grad 中\n",
    "        with torch.no_grad():\n",
    "            outputs = pipeline(\n",
    "                prompt,\n",
    "                num_images_per_prompt=num_images,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                generator=generator\n",
    "            )\n",
    "        \n",
    "        # 保存所有生成的图像\n",
    "        output_dir = os.path.dirname(output_image_path)\n",
    "        if output_dir:\n",
    "             os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        saved_paths = []\n",
    "        if num_images == 1:\n",
    "            outputs.images[0].save(output_image_path)\n",
    "            print(f\"图像已保存到 {output_image_path}\")\n",
    "            saved_paths.append(output_image_path)\n",
    "        else:\n",
    "            base_path, extension = os.path.splitext(output_image_path)\n",
    "            if not extension:\n",
    "                 extension = \".png\" # 默认扩展名\n",
    "            for i, image in enumerate(outputs.images):\n",
    "                path = f\"{base_path}_{i}{extension}\"\n",
    "                image.save(path)\n",
    "                saved_paths.append(path)\n",
    "            print(f\"已保存 {num_images} 张图像到 {output_dir} (以 {base_path}_n{extension} 格式)\")\n",
    "        \n",
    "        # 清理显存\n",
    "        del pipeline\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return outputs.images, saved_paths\n",
    "    except Exception as e:\n",
    "        print(f\"推理过程中发生错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # 清理显存\n",
    "        if 'pipeline' in locals():\n",
    "             del pipeline\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        return None, []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8e86f",
   "metadata": {},
   "source": [
    "## 5. 配置和执行\n",
    "\n",
    "在此单元格中设置参数，然后运行它以及后续的训练或推理单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ff0b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择适合低资源设备的小型模型...\n",
      "已选择模型: CompVis/stable-diffusion-v1-4\n",
      "此模型与较旧版本的diffusers兼容性更好\n",
      "模型将在首次使用时自动从Hugging Face下载\n",
      "\n",
      "将使用的预训练模型: CompVis/stable-diffusion-v1-4\n",
      "输出目录: ./dreambooth_output\n",
      "\n",
      "--- 训练模式已启用 ---\n",
      "实例图像目录: ./instance_images\n",
      "类别提示词: a photo of a dog\n",
      "实例提示词模板: a photo of a {} dog\n",
      "训练步数: 800\n",
      "学习率: 5e-06\n",
      "训练文本编码器: True\n",
      "先验保留样本数: 50\n"
     ]
    }
   ],
   "source": [
    "# --- 配置参数 ---\n",
    "\n",
    "# 操作模式 (选择一个)\n",
    "train_mode = True\n",
    "infer_mode = False\n",
    "\n",
    "# 模型设置\n",
    "use_small_model = True # 是否自动选择小型预训练模型\n",
    "pretrained_model_name_or_path = None # 如果 use_small_model=False，在此指定模型名称或路径\n",
    "output_dir = \"./dreambooth_output\" # 训练后的模型和类别图像保存路径 / 推理时加载模型的路径\n",
    "\n",
    "# 训练数据设置 (仅训练时需要)\n",
    "instance_data_dir = \"./instance_images\" # 包含你的训练图片 (例如 5-10 张 .jpg 或 .png)\n",
    "class_prompt = \"a photo of a dog\" # 你的训练对象所属的类别 (例如 \"a photo of a dog\", \"a painting of a landscape\")\n",
    "instance_prompt_template = \"a photo of a {} dog\" # 实例提示词模板, {} 会被稀有令牌替换\n",
    "\n",
    "# 训练参数 (仅训练时需要)\n",
    "learning_rate = 5e-6\n",
    "max_train_steps = 800\n",
    "prior_preservation_weight = 1.0 # 先验保留损失权重 (1.0 通常效果好)\n",
    "prior_generation_samples = 50 # 生成的类别图像数量 (显存小时可减少)\n",
    "train_text_encoder = True # 是否同时训练文本编码器 (需要更多显存，但通常效果更好)\n",
    "mixed_precision = \"fp16\" # 使用混合精度 ('fp16', 'bf16', or 'no')\n",
    "seed_train = 42\n",
    "\n",
    "# 推理参数 (仅推理时需要)\n",
    "prompt_template_infer = \"a photo of a {} dog running on the beach\" # 推理时使用的提示词模板\n",
    "output_image_path = \"./generated_dreambooth.png\" # 生成图像的保存路径\n",
    "num_images_to_generate = 1 # 要生成的图像数量\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "seed_infer = None # 推理种子 (None 表示随机)\n",
    "\n",
    "# --- 参数处理 ---\n",
    "if use_small_model or pretrained_model_name_or_path is None:\n",
    "    pretrained_model_name_or_path = download_small_model()\n",
    "\n",
    "print(f\"\\n将使用的预训练模型: {pretrained_model_name_or_path}\")\n",
    "print(f\"输出目录: {output_dir}\")\n",
    "\n",
    "if train_mode:\n",
    "    print(\"\\n--- 训练模式已启用 ---\")\n",
    "    print(f\"实例图像目录: {instance_data_dir}\")\n",
    "    print(f\"类别提示词: {class_prompt}\")\n",
    "    print(f\"实例提示词模板: {instance_prompt_template}\")\n",
    "    print(f\"训练步数: {max_train_steps}\")\n",
    "    print(f\"学习率: {learning_rate}\")\n",
    "    print(f\"训练文本编码器: {train_text_encoder}\")\n",
    "    print(f\"先验保留样本数: {prior_generation_samples}\")\n",
    "    # 检查实例目录是否存在\n",
    "    if not os.path.exists(instance_data_dir):\n",
    "        print(f\"\\n错误: 实例图像目录 '{instance_data_dir}' 不存在！\")\n",
    "        print(\"请创建该目录并将您的训练图像放入其中。\")\n",
    "        # 可以在这里创建目录，但用户仍需放入图片\n",
    "        try:\n",
    "            os.makedirs(instance_data_dir)\n",
    "            print(f\"已创建目录: {instance_data_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"创建目录失败: {e}\")\n",
    "        raise FileNotFoundError(f\"实例图像目录 '{instance_data_dir}' 不存在或为空。\")\n",
    "    elif not any(f.lower().endswith(('.png', '.jpg', '.jpeg')) for f in os.listdir(instance_data_dir)):\n",
    "         print(f\"\\n警告: 实例图像目录 '{instance_data_dir}' 为空或不包含支持的图像文件 (.png, .jpg, .jpeg)。\")\n",
    "         raise FileNotFoundError(f\"实例图像目录 '{instance_data_dir}' 为空或不包含图像。\")\n",
    "\n",
    "if infer_mode:\n",
    "    print(\"\\n--- 推理模式已启用 ---\")\n",
    "    print(f\"将从以下路径加载模型: {output_dir}\")\n",
    "    print(f\"推理提示词模板: {prompt_template_infer}\")\n",
    "    print(f\"输出图像路径: {output_image_path}\")\n",
    "\n",
    "if not train_mode and not infer_mode:\n",
    "    show_quick_help()\n",
    "    print(\"\\n请在上方配置单元格中设置 train_mode=True 或 infer_mode=True。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d6373",
   "metadata": {},
   "source": [
    "## 6. 执行训练或推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efcfc3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始 DreamBooth 训练...\n",
      "\n",
      "已检测到GPU: NVIDIA GeForce RTX 4070 Laptop GPU (8.0GB)\n",
      "警告: 显存可能不足以同时训练文本编码器。如果遇到 OOM 错误，请尝试设置 train_text_encoder = False。\n",
      "选中的稀有令牌标识符: 'ier'\n",
      "实例提示词: 'a photo of a ier dog'\n",
      "类别提示词: 'a photo of a dog'\n",
      "加载预训练模型: CompVis/stable-diffusion-v1-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 28.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# 在Notebook中通常不强制退出，让用户决定是否继续\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# use_cpu = input(\"是否继续在CPU上训练? [y/n]: \").lower()\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# if use_cpu != 'y':\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m#     print(\"已取消训练。请在GPU环境下运行。\")\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m#     # raise RuntimeError(\"需要GPU进行训练\") # 或者抛出错误\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     trained_identifier = \u001b[43mdreambooth_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstance_data_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstance_data_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstance_prompt_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstance_prompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_train_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_train_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprior_preservation_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior_preservation_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprior_generation_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior_generation_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_text_encoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_text_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmixed_precision\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m训练完成！\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m模型已保存到: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mdreambooth_training\u001b[39m\u001b[34m(pretrained_model_name_or_path, instance_data_dir, output_dir, class_prompt, instance_prompt_template, learning_rate, max_train_steps, prior_preservation_weight, prior_generation_samples, gradient_accumulation_steps, train_text_encoder, train_batch_size, seed, mixed_precision)\u001b[39m\n\u001b[32m     70\u001b[39m      unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path)\n\u001b[32m     71\u001b[39m      pipeline_for_gen = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mpipeline_for_gen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# 生成类别图像用于先验保留\u001b[39;00m\n\u001b[32m     76\u001b[39m class_images_dir = os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mclass_images\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:482\u001b[39m, in \u001b[36mDiffusionPipeline.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m     module.to(device=device)\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_group_offloaded:\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    485\u001b[39m     module.dtype == torch.float16\n\u001b[32m    486\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    487\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[32m    488\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[32m    489\u001b[39m ):\n\u001b[32m    490\u001b[39m     logger.warning(\n\u001b[32m    491\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    492\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    495\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py:1353\u001b[39m, in \u001b[36mModelMixin.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1348\u001b[39m     logger.warning(\n\u001b[32m   1349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is group offloaded and moving it using `.to()` is not supported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1350\u001b[39m     )\n\u001b[32m   1351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 915 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xy\\miniconda3\\envs\\dreambooth\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if train_mode:\n",
    "    print(\"\\n开始 DreamBooth 训练...\")\n",
    "    # 检查GPU并优化设置\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_info = torch.cuda.get_device_name()\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"\\n已检测到GPU: {gpu_info} ({gpu_memory:.1f}GB)\")\n",
    "        \n",
    "        # 根据GPU内存调整优化参数 (示例)\n",
    "        if gpu_memory < 6 and prior_generation_samples > 30:  # 低端GPU\n",
    "            print(f\"警告: 检测到低显存GPU ({gpu_memory:.1f}GB)。当前先验样本数为 {prior_generation_samples}。\")\n",
    "            print(\"建议减少先验图像数量 (例如 prior_generation_samples = 30) 以避免内存不足。\")\n",
    "            # 可以选择自动调整或提示用户\n",
    "            # prior_generation_samples = 30\n",
    "            # print(\"已自动调整先验图像数量为 30\")\n",
    "        if gpu_memory < 8 and train_text_encoder:\n",
    "             print(f\"警告: 显存可能不足以同时训练文本编码器。如果遇到 OOM 错误，请尝试设置 train_text_encoder = False。\")\n",
    "    else:\n",
    "        print(\"\\n警告: 未检测到GPU! 训练将在CPU上运行，这会非常慢!\")\n",
    "        # 在Notebook中通常不强制退出，让用户决定是否继续\n",
    "        # use_cpu = input(\"是否继续在CPU上训练? [y/n]: \").lower()\n",
    "        # if use_cpu != 'y':\n",
    "        #     print(\"已取消训练。请在GPU环境下运行。\")\n",
    "        #     # raise RuntimeError(\"需要GPU进行训练\") # 或者抛出错误\n",
    "            \n",
    "    try:\n",
    "        trained_identifier = dreambooth_training(\n",
    "            pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "            instance_data_dir=instance_data_dir,\n",
    "            output_dir=output_dir,\n",
    "            class_prompt=class_prompt,\n",
    "            instance_prompt_template=instance_prompt_template,\n",
    "            learning_rate=learning_rate,\n",
    "            max_train_steps=max_train_steps,\n",
    "            prior_preservation_weight=prior_preservation_weight,\n",
    "            prior_generation_samples=prior_generation_samples,\n",
    "            train_text_encoder=train_text_encoder,\n",
    "            seed=seed_train,\n",
    "            mixed_precision=mixed_precision\n",
    "        )\n",
    "        print(\"\\n训练完成！\")\n",
    "        print(f\"模型已保存到: {output_dir}\")\n",
    "        print(f\"学习到的标识符: {trained_identifier}\")\n",
    "        print(f\"现在可以将 infer_mode 设置为 True，并使用提示词模板 '{prompt_template_infer.format(trained_identifier)}' 进行推理。\")\n",
    "    except Exception as e:\n",
    "         print(f\"\\n训练过程中发生错误: {e}\")\n",
    "         import traceback\n",
    "         traceback.print_exc()\n",
    "         print(\"\\n请检查错误信息、参数配置和依赖项安装。\")\n",
    "\n",
    "elif infer_mode:\n",
    "    print(\"\\n开始 DreamBooth 推理...\")\n",
    "    # 添加CUDA检测\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cpu\":\n",
    "        print(\"\\n警告: 未检测到可用的CUDA GPU。将使用CPU运行，这会非常慢!\")\n",
    "        # 在Notebook中通常不强制退出\n",
    "    else:\n",
    "        gpu_info = torch.cuda.get_device_name()\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"\\n成功检测到GPU: {gpu_info} ({gpu_memory:.1f}GB)\")\n",
    "        print(f\"将使用设备: {device}\")\n",
    "\n",
    "    generated_images, saved_paths = inference(\n",
    "        model_path=output_dir, # 推理时加载训练好的模型\n",
    "        prompt_template=prompt_template_infer,\n",
    "        class_prompt=class_prompt, # 用于查找 identifier.txt\n",
    "        # identifier=None, # 通常让函数自动从文件加载\n",
    "        output_image_path=output_image_path,\n",
    "        num_images=num_images_to_generate,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        seed=seed_infer\n",
    "    )\n",
    "    \n",
    "    if generated_images:\n",
    "        print(\"\\n推理完成！\")\n",
    "        # 可选：在Notebook中显示生成的图像\n",
    "        from IPython.display import display\n",
    "        for img_path in saved_paths:\n",
    "             try:\n",
    "                 display(Image.open(img_path))\n",
    "             except Exception as display_e:\n",
    "                  print(f\"无法显示图像 {img_path}: {display_e}\")\n",
    "    else:\n",
    "        print(\"\\n推理失败。请检查错误信息。\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n没有选择操作模式。请在配置单元格中设置 train_mode=True 或 infer_mode=True，然后重新运行该单元格和此执行单元格。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreambooth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
